import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from colorama import Fore, Style, init
import pyfiglet
import os
import time
import sys
import webbrowser
from collections import defaultdict
import random

init()

# ------------------- UPDATE CHECK -------------------

def check_for_update():
    update_url = "https://raw.githubusercontent.com/0-d3y/DarkCrawler/refs/heads/main/update.txt"
    local_update_file = "update.txt"

    try:
        response = requests.get(update_url, timeout=10)
        response.raise_for_status()
        remote_content = response.text.strip()
    except Exception as e:
        print(Fore.RED + f"[!] Failed to fetch update info: {e}" + Style.RESET_ALL)
        return

    local_content = ""
    if os.path.exists(local_update_file):
        with open(local_update_file, "r", encoding="utf-8") as f:
            local_content = f.read().strip()

    if remote_content == local_content:
        print(Fore.GREEN + "[+] No updates available. You are using the latest version." + Style.RESET_ALL)
    else:
        print(Fore.YELLOW + "[*] Update available!" + Style.RESET_ALL)
        choice = input(Fore.YELLOW + "Do you want to update now? (y/n): " + Style.RESET_ALL).strip().lower()
        if choice == 'y':
            perform_update(remote_content, local_update_file)
        else:
            print(Fore.CYAN + "Update cancelled by user." + Style.RESET_ALL)

def perform_update(remote_content, local_update_file):
    # For now, just update the local update.txt file to simulate update
    # Real update logic can be added here (e.g., download new files, replace code, etc.)
    try:
        with open(local_update_file, "w", encoding="utf-8") as f:
            f.write(remote_content)
        print(Fore.GREEN + "[+] Update applied successfully." + Style.RESET_ALL)
    except Exception as e:
        print(Fore.RED + f"[!] Failed to apply update: {e}" + Style.RESET_ALL)


visited_links = set()
file_structure = []

import threading
import sys

progress_lock = threading.Lock()
urls_to_visit = set()
urls_visited_count = 0
total_urls_discovered = 0  # Start with zero, will increment when base URL added

def print_progress_bar(percentage, length=40):
    filled_length = int(length * percentage // 100)
    bar = '‚ñà' * filled_length + '-' * (length - filled_length)
    sys.stdout.write(f'\r{Fore.GREEN}[{bar}] {percentage:.1f}%{Style.RESET_ALL}')
    sys.stdout.flush()

common_admin_paths = [
    '/admin', '/administrator', '/login', '/admin/login', '/user/login',
    '/adminpanel', '/admin_area', '/admin_area/login', '/cpanel', '/controlpanel',
    '/manage', '/backend', '/moderator', '/moderator/login', '/admin1', '/admin2',
    '/admin3', '/admin4', '/admin5', '/admin.php', '/admin.html', '/admin.asp',
    '/admin.aspx', '/admin.cgi', '/admin.jsp', '/admin_panel', '/admin-console',
    '/wp-admin', '/wp-login.php', '/wp-login', '/wp-admin/admin.php', '/wp-admin/admin-ajax.php',
    '/wp-content', '/wp-includes', '/wp-json', '/wp-login.php?action=register',
    '/administrator', '/administrator/index.php', '/administrator/login.php',
    '/user/login', '/user/register', '/user/password',
    '/admin', '/admin/dashboard', '/admin/login',
    '/admin123', '/admin-dev', '/admin-dev/index.php',
    '/login.php', '/adminpanel.php', '/admin_area.php', '/cpanel.php', '/controlpanel.php',
    '/manage.php', '/backend.php', '/moderator.php', '/moderator/login.php',
    '/admin1.php', '/admin2.php', '/admin3.php', '/admin4.php', '/admin5.php',
    '/admin_console', '/admin-console.php', '/adminpanel/login.php', '/admin_area/login.php',
    '/cpanel/login.php', '/controlpanel/login.php', '/manage/login.php', '/backend/login.php',
    '/moderator/login.php', '/admin_login.php', '/admin_login', '/admin_login.php',
    '/adminpanel/login', '/admin_area/login', '/cpanel/login', '/controlpanel/login',
    '/manage/login', '/backend/login', '/moderator/login', '/admin_login',
    '/adminpanel/login', '/admin_area/login', '/cpanel/login', '/controlpanel/login',
    '/manage/login', '/backend/login', '/moderator/login', '/admin_login',
    '/admin.php', '/admin.html', '/admin.asp', '/admin.aspx', '/admin.cgi', '/admin.jsp',
    '/admin_panel', '/admin-console', '/admin-console.php', '/adminpanel.php', '/adminpanel/login.php',
    '/admin_area.php', '/admin_area/login.php', '/cpanel.php', '/cpanel/login.php',
    '/controlpanel.php', '/controlpanel/login.php', '/manage.php', '/manage/login.php',
    '/backend.php', '/backend/login.php', '/moderator.php', '/moderator/login.php',
    '/admin1.php', '/admin2.php', '/admin3.php', '/admin4.php', '/admin5.php',
    '/admin_login.php', '/admin_login', '/admin_login.php'
]

admin_paths_found = set()


try:
    os.mkdir('Results')
except:
    pass


# ------------------- ANIMATION FUNCTIONS -------------------

def type_writer(text, speed=0.01, color=Fore.WHITE, end='\n'):
    for char in text:
        sys.stdout.write(color + char + Style.RESET_ALL)
        sys.stdout.flush()
        time.sleep(speed)
    sys.stdout.write(end)
    sys.stdout.flush()

def loading_animation(text, dots=3, delay=0.4):
    for _ in range(dots):
        sys.stdout.write(Fore.CYAN + text + "." * (_ + 1) + Style.RESET_ALL + "\r")
        sys.stdout.flush()
        time.sleep(delay)
    print(" " * len(text) + "\r", end="")

def print_logo():
    os.system("cls" if os.name == "nt" else "clear")
    logo = pyfiglet.figlet_format("DarkCrawler")
    type_writer(logo, 0.005, Fore.CYAN)
    type_writer("[::] Deep Web File Mapper | by S4Tech | Mr.SaMi", 0.03, Fore.MAGENTA)
    type_writer("-" * 65, 0.001, Fore.LIGHTBLACK_EX)

# ------------------- FILE ICONS -------------------

def get_file_icon(filename):
    ext = os.path.splitext(filename)[1].lower()
    icons = {
        '.html': 'üåê', '.htm': 'üåê', '.php': 'üêò', '.asp': 'üÖ∞Ô∏è',
        '.js': 'üìú', '.css': 'üé®', '.png': 'üñºÔ∏è', '.jpg': 'üñºÔ∏è',
        '.jpeg': 'üñºÔ∏è', '.gif': 'üñºÔ∏è', '.svg': 'üñºÔ∏è', '.webp': 'üñºÔ∏è',
        '.json': 'üîñ', '.txt': 'üìù', '.xml': 'üì∞', '.ico': 'üñºÔ∏è',
        '.pdf': 'üìÑ', '.doc': 'üìÑ', '.docx': 'üìÑ', '.xls': 'üìä',
        '.xlsx': 'üìä', '.ppt': 'üìä', '.pptx': 'üìä', '.zip': 'üóúÔ∏è',
        '.rar': 'üóúÔ∏è', '.tar': 'üóúÔ∏è', '.gz': 'üóúÔ∏è', '.mp3': 'üéµ',
        '.wav': 'üéµ', '.mp4': 'üé¨', '.avi': 'üé¨', '.sql': 'üóÉÔ∏è',
        '.db': 'üóÑÔ∏è', '.exe': '‚öôÔ∏è', '.dll': 'üîß', '.py': 'üêç'
    }
    return icons.get(ext, 'üìÑ')

# ------------------- CRAWLER -------------------

def is_valid_file(url):
    exts = ['.html', '.htm', '.php', '.asp', '.js', '.css', '.png', '.jpg', 
            '.jpeg', '.gif', '.svg', '.webp', '.json', '.txt', '.xml', '.ico',
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.zip',
            '.rar', '.tar', '.gz', '.mp3', '.wav', '.mp4', '.avi', '.sql',
            '.db', '.exe', '.dll', '.py']
    return any(url.lower().endswith(ext) for ext in exts)

import threading

def crawl(url, base_url, depth=0, max_depth=7):
    global urls_visited_count, urls_to_visit, total_urls_discovered
    if depth > max_depth or url in visited_links:
        return
    with progress_lock:
        if url in urls_to_visit:
            urls_to_visit.remove(url)
        urls_visited_count += 1
        total_known = urls_visited_count + len(urls_to_visit)
        percentage = (urls_visited_count / total_known) * 100 if total_known > 0 else 100
        print_progress_bar(percentage)
    visited_links.add(url)
    #print(f"Visiting: {url}")  # Debug log

    import requests.exceptions

    max_retries = 3
    for attempt in range(max_retries):
        try:
            res = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            content_type = res.headers.get("Content-Type", "")
            if "text/html" not in content_type:
                return
            break
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
            #print(f"Attempt {attempt+1} failed to fetch {url}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
            else:
                return
        except Exception as e:
            #print(f"Failed to fetch {url}: {e}")
            return

    soup = BeautifulSoup(res.text, "html.parser")
    parsed_url = urlparse(url)
    path = parsed_url.path.strip("/")
    if path:
        # Check for duplicate file in same path
        if not any(existing_path == (path + ("/" if url.endswith("/") else "")) for existing_path, _ in file_structure):
            file_structure.append((path + ("/" if url.endswith("/") else ""), url))

    # Check if current path is an admin path
    for admin_path in common_admin_paths:
        if path.rstrip('/').lower() == admin_path.rstrip('/').lower():
            admin_paths_found.add(url)
            break

    threads = []
    for tag in soup.find_all(['a', 'link', 'script', 'img']):
        attr = 'href' if tag.name in ['a', 'link'] else 'src'
        link = tag.get(attr)
        if not link:
            continue
        full_url = urljoin(base_url, link)
        if not full_url.startswith(base_url):
            continue
        with progress_lock:
            if full_url not in urls_to_visit and full_url not in visited_links:
                urls_to_visit.add(full_url)
                total_urls_discovered += 1
                remaining = len(urls_to_visit)
                total = urls_visited_count + remaining
                percentage = (urls_visited_count / total) * 100 if total > 0 else 100
                print_progress_bar(percentage)

        parsed = urlparse(full_url)
        clean_path = parsed.path.strip("/")
        if clean_path:
            if is_valid_file(clean_path):
                # Check for duplicate file in same path
                if not any(existing_path == clean_path for existing_path, _ in file_structure):
                    file_structure.append((clean_path, full_url))
            elif full_url not in visited_links:
                with progress_lock:
                    urls_to_visit.add(full_url)
                # Removed random delay to speed up crawling
                t = threading.Thread(target=crawl, args=(full_url, base_url, depth + 1, max_depth))
                t.start()
                threads.append(t)
    for t in threads:
        t.join()

# ------------------- TREE PRINTER -------------------

def build_tree():
    tree = {'__files__': [], '__subdirs__': defaultdict(dict)}
    for path, url in file_structure:
        parts = path.split('/')
        current = tree
        for part in parts[:-1]:
            current = current['__subdirs__'].setdefault(part, {'__files__': [], '__subdirs__': defaultdict(dict)})
        if '.' in parts[-1]:  # It's a file
            # Avoid duplicate files in the same directory
            if not any(f[0] == parts[-1] for f in current['__files__']):
                current['__files__'].append((parts[-1], url))
        else:  # It's a directory
            if parts[-1] not in current['__subdirs__']:
                current['__subdirs__'][parts[-1]] = {'__files__': [], '__subdirs__': defaultdict(dict)}
    return tree

def print_tree(node, prefix='', is_last=False, is_root=False):
    fast_speed = 0.001
    if is_root:
        type_writer('üåê ' + parsed_base.netloc, fast_speed, Fore.BLUE)
        prefix = ''
    
    # Print files first
    for i, (filename, url) in enumerate(sorted(node['__files__'], key=lambda x: x[0])):
        is_file_last = i == len(node['__files__']) - 1 and not node['__subdirs__']
        connector = '‚îî‚îÄ‚îÄ ' if is_file_last else '‚îú‚îÄ‚îÄ '
        icon = get_file_icon(filename)
        line = prefix + connector + icon + ' ' + filename
        type_writer(line, fast_speed)
    
    # Print subdirectories
    keys = sorted(node['__subdirs__'].keys())
    for i, key in enumerate(keys):
        is_dir_last = i == len(keys) - 1
        connector = '‚îî‚îÄ‚îÄ ' if is_dir_last else '‚îú‚îÄ‚îÄ '
        line = prefix + connector + 'üìÅ ' + key
        type_writer(line, fast_speed, Fore.BLUE)
        new_prefix = prefix + ('    ' if is_dir_last else '‚îÇ   ')
        print_tree(node['__subdirs__'][key], new_prefix, is_dir_last)

# ------------------- HTML SAVER -------------------

def save_to_html(url,line ,seconds):
    url = url.replace('https://','').replace('http://','').replace('/','')
    tree = build_tree()
    html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>DarkCrawler Results - {url}</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=swap');
        body {{
            background-color: #0d0d0d;
            color: #00ff00;
            font-family: 'Share Tech Mono', monospace;
            margin: 20px;
            line-height: 1.4;
            white-space: pre-wrap;
            word-wrap: break-word;
        }}
        h1 {{
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 0.5em;
            text-shadow: 0 0 10px #00ff00;
        }}
        .terminal-window {{
            background-color: #111;
            border-radius: 10px;
            box-shadow: 0 0 20px #00ff00;
            max-width: 900px;
            margin: 0 auto;
            font-family: 'Share Tech Mono', monospace;
            color: #00ff00;
            overflow: hidden;
            display: flex;
            flex-direction: column;
        }}
        .terminal-header {{
            background: #222;
            padding: 8px 12px;
            display: flex;
            align-items: center;
            gap: 8px;
            border-bottom: 2px solid #00ff00;
        }}
        .terminal-btn {{
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background-color: #ff5f56;
            box-shadow: 0 0 2px #ff5f56;
            cursor: pointer;
        }}
        .terminal-btn.minimize {{
            background-color: #ffbd2e;
            box-shadow: 0 0 2px #ffbd2e;
        }}
        .terminal-btn.maximize {{
            background-color: #27c93f;
            box-shadow: 0 0 2px #27c93f;
        }}
        .terminal-content {{
            background-color: #111;
            padding: 20px;
            overflow-x: auto;
            font-size: 1.1em;
            white-space: pre;
            flex-grow: 1;
        }}
        .folder {{
            color: #00ff00;
            font-weight: bold;
        }}
        .file {{
            color: #00cc00;
        }}
        a {{
            color: #00ff00;
            text-decoration: none;
        }}
        a:hover {{
            text-decoration: underline;
            color: #33ff33;
        }}
        .admin-path {{
            color: #ff3300;
            font-weight: bold;
            text-shadow: 0 0 5px #ff3300;
        }}
        footer {{
            margin-top: 40px;
            text-align: center;
            font-size: 0.9em;
            color: #007700;
            text-shadow: 0 0 5px #007700;
        }}
        footer a {{
            color: #00ff00;
            text-decora
            font-weight: bold;
        }}
        footer a:hover {{
            color: #33ff33;
            text-decoration: underline;
        }}
        /* Terminal style enhancements */
        .tree span {{
            display: block;
            line-height: 1.4em;
        }}
        .tree .folder::before {{
            content: "üìÅ ";
        }}
        .tree .file::before {{
            content: "üìÑ ";
        }}
        .tree a {{
            font-family: 'Share Tech Mono', monospace;
        }}
    </style>
</head>
<body>
    <h1>DarkCrawler Results</h1>
    <div class="tree" role="tree" aria-label="Website structure">
    <h4 class='admin-path' aria-live="polite" aria-atomic="true">
[i] Total items found: {line}
[i] Time taken: {seconds} seconds
    </h4>
    <h4 class='admin-path'>üåê {url}</h4>
    </div>
    <div class="terminal-window" role="region" aria-label="Terminal window displaying site map">
        <div class="terminal-header" aria-hidden="true">
            <div class="terminal-btn close"></div>
            <div class="terminal-btn minimize"></div>
            <div class="terminal-btn maximize"></div>
        </div>
        <pre class="terminal-content tree" id="terminalContent" role="tree" aria-label="Website structure">
"""
    tree_lines = generate_html_tree(tree)
    html_content += '\n'.join(tree_lines)
    if admin_paths_found:
        html_content += "\n‚ö†Ô∏è Admin Paths Found:\n"
        for path in sorted(admin_paths_found):
            html_content += f"{path}\n"
    html_content += """
        </pre>
    </div>
    <footer>
        &copy; 2024 DarkCrawler by <a href="https://github.com/0-d3y" | Mr.SaMi" target="_blank">S4Tech | Mr.SaMi</a> 
    </footer>
<script>
const terminalContent = document.getElementById('terminalContent');
const lines = terminalContent.textContent.split('\\n');
terminalContent.textContent = '';
let index = 0;
function typeLine() {
  if (index < lines.length) {
    let line = lines[index];
    let i = 0;
    let interval = setInterval(() => {
      terminalContent.textContent += line.charAt(i);
      i++;
      if (i >= line.length) {
        clearInterval(interval);
        terminalContent.textContent += '\\n';
        index++;
        setTimeout(typeLine, 50);
      }
      }, 5);
  }
}
window.onload = () => {
  typeLine();
};
</script>
</body>
</html>
"""
    filename = f"./Results/DarkCrawler_results - {url}.html"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(html_content)
    return filename

def generate_html_tree(node, level=0, prefix=''):
    lines = []
    keys = sorted(node['__subdirs__'].keys())
    files = sorted(node['__files__'], key=lambda x: x[0])
    total_items = len(keys) + len(files)
    for i, key in enumerate(keys):
        is_last = i == total_items - 1 and len(files) == 0
        connector = '‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '
        lines.append(f"{prefix}{connector}üìÅ {key}")
        extension = '    ' if is_last else '‚îÇ   '
        lines.extend(generate_html_tree(node['__subdirs__'][key], level + 1, prefix + extension))
    for i, (filename, url) in enumerate(files):
        is_last = i == len(files) - 1
        connector = '‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '
        icon = get_file_icon(filename)
        lines.append(f"{prefix}{connector}{icon} {filename}")
    return lines

# ------------------- MAIN -------------------

def extract_file_structure():
    type_writer("\n=== Main Menu ===", speed=0.03, color=Fore.CYAN)
    type_writer("  [1] Extract File Structure", speed=0.03)
    type_writer("  [2] About the Tool", speed=0.03)
    type_writer("  [3] About the Developer", speed=0.03)
    type_writer("  [0] Exit", speed=0.03)

def about_tool():
    type_writer("About DarkCrawler:", speed=0.03, color=Fore.GREEN)
    type_writer("Advanced web crawler that extracts and visualizes website structures.", speed=0.03)
    input("\nPress Enter to return to the menu...")

def about_developer():
    type_writer("About the Developer:", speed=0.03, color=Fore.GREEN)
    type_writer("Created by S4Tech | Mr.SaMi", speed=0.03)
    input("\nPress Enter to return to the menu...")

def main_menu():
    while True:
        type_writer("\n=== Main Menu ===", speed=0.03, color=Fore.CYAN)
        type_writer("  [1] Extract File Structure", speed=0.03)
        type_writer("  [2] About the Tool", speed=0.03)
        type_writer("  [3] About the Developer", speed=0.03)
        type_writer("  [4] Update Now", speed=0.03)
        type_writer("  [0] Exit", speed=0.03)

        choice = input(Fore.YELLOW + "\nEnter choice: " + Style.RESET_ALL).strip()
        if choice == '1':
            run_crawl()
        elif choice == '2':
            about_tool()
        elif choice == '3':
            about_developer()
        elif choice == '4':
            check_for_update()
        elif choice == '0':
            type_writer("Exiting DarkCrawler. Goodbye!", speed=0.03, color=Fore.RED)
            break
        else:
            type_writer("Invalid choice. Please try again.", speed=0.03, color=Fore.RED)

def run_crawl():
    global visited_links, file_structure, admin_paths_found, urls_visited_count, urls_to_visit
    visited_links = set()
    file_structure = []
    admin_paths_found = set()
    urls_visited_count = 0
    urls_to_visit = set()

    base_url = input(Fore.YELLOW + "[+] Enter URL => " + Style.RESET_ALL).strip()
    if not base_url.startswith(('http://', 'https://')):
        base_url = 'https://' + base_url
    if not base_url.endswith("/"):
        base_url += "/"
    global parsed_base
    parsed_base = urlparse(base_url)

    loading_animation("[*] Initiating deep scan")

    # Initialize urls_to_visit with base_url
    urls_to_visit.add(base_url)

    start_time = time.time()
    crawl(base_url, base_url)
    elapsed = time.time() - start_time

    # Ensure progress bar shows 100% at end
    print_progress_bar(100)
    print()

    type_writer("\n[+] Scan completed successfully!", 0.03, Fore.LIGHTGREEN_EX)
    type_writer(f"[i] Total items found: {len(file_structure)}", 0.02, Fore.MAGENTA)
    type_writer(f"[i] Time taken: {elapsed:.2f} seconds", 0.02, Fore.MAGENTA)

    type_writer("\n=== Website Structure ===\n", 0.01, Fore.LIGHTBLACK_EX)
    print_tree(build_tree(), is_root=True)

    # Save to HTML
    html_file = save_to_html(base_url, len(file_structure), f"{elapsed:.2f}")
    type_writer(f"\n[+] Results saved to {html_file}", 0.03, Fore.GREEN)
    if input("Open in browser? (y/n): ").lower() == 'y':
        webbrowser.open('file:///' + os.path.abspath(html_file).replace('\\', '/'))

if __name__ == "__main__":
    print_logo()
    main_menu()
